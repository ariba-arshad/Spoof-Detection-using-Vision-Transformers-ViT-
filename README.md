# Spoof Detection using Vision Transformers (ViT)

Fine-tuned the pre-trained google/vit-base-patch16-224 Vision Transformer on the CelebA dataset (~66,000 samples) for binary classification of real vs. spoofed facial images. Achieved a training and validation loss of 0.01, with a recall of 0.9984.

📁 Datasets Used

“nguyenkhoa/celeba-spoof-for-face-antispoofing-test” dataset
- Training samples: 37,400
- Validation samples: 9,350
- Testing samples: 20,037

📊 Evaluation Metrics
- Training loss: 0.01
- Validation loss: 0.01
- Accuracy: 0.9984
- Precision: 0.9984
- Recall: 0.9984
- F1 score: 0.9984
